#   H0: (beta1, beta2) == (0, 0) vs H1: (beta1, beta2) != (0, 0)
linearHypothesis(fm, rbind(c(0,1,0), c(0,0,1)), c(0,0))
summary(fm)
p <- 2  # number of tested coefficients
r <- 2  # number of regressors
# Confidence region:
# center: point estimate
c(coefficients(fm)[2], coefficients(fm)[3])
# Direction of the axes?
eigen(vcov(fm)[2:3,2:3])$vectors
plot(coefficients(fm)[2], coefficients(fm)[3], xlim = c(-6,6), ylim = c(-6,6), asp=1, xlab='beta1', ylab='beta2')
ellipse(coefficients(fm)[2:3], vcov(fm)[2:3,2:3], sqrt(p*qf(1-0.05,p,n-(r+1))))
abline(v=0)
abline(h=0)
# Note: colinearity!
# Bonferroni intervals (level 95%)
Bf <- rbind(
beta1=c(coefficients(fm)[2]-sqrt(vcov(fm)[2,2])*qt(1-0.05/(2*p), n-(r+1)),
coefficients(fm)[2]+sqrt(vcov(fm)[2,2])*qt(1-0.05/(2*p), n-(r+1))),
beta2=c(coefficients(fm)[3]-sqrt(vcov(fm)[3,3])*qt(1-0.05/(2*p), n-(r+1)),
coefficients(fm)[3]+sqrt(vcov(fm)[3,3])*qt(1-0.05/(2*p), n-(r+1)))
)
Bf
# or (only for intervals on beta)
confint(fm, level= 1-0.05/p)[2:3,]  # Bonferroni correction!
# Note: confint() returns the confidence intervals one-at-a-time;
# to have a global level 95% we need to include a correction
### Test:
# H0: (beta0+beta2, beta1) == (0,0) vs H1: (beta0+beta2, beta1) != (0,0)
C <- rbind(c(1,0,1), c(0,1,0))
linearHypothesis(fm, C, c(0,0))
# Homework
# Build the associated confidence region
# Confidence intervals for the mean
# & prediction (new obs)
# Assumption: Eps ~ N(0, sigma^2)
## Prediction ---------------------------------------------------------------------------------
# Command predict()
Z0.new <- data.frame(speed1=10, speed2=10^2)
# Conf. int. for the mean
Conf <- predict(fm, Z0.new, interval='confidence', level=1-0.05)
Conf
# Pred. int. for a new obs
Pred <- predict(fm, Z0.new, interval='prediction', level=1-0.05)
Pred
plot(cars, xlab='Speed', ylab='Stopping distance', las=1, xlim=c(0,30), ylim=c(-10,130))
x <- seq(0,30,by=0.1)
b <- coef(fm)
lines(x, b[1]+b[2]*x+b[3]*x^2)
points(10,Conf[1], pch=19)
segments(10,Pred[2],10,Pred[3],col='gold', lwd=2)
segments(10,Conf[2],10,Conf[3],col='red', lwd=2)
points(10,Conf[2], pch='-', col='red', lwd=2)
points(10,Conf[3], pch='-', col='red', lwd=2)
points(10,Pred[2], pch='-', col='gold', lwd=2)
points(10,Pred[3], pch='-', col='gold', lwd=2)
# We can repeat these for values of speed between 0 and 30
# (point-wise intervals!)
Z0   <- data.frame(cbind(speed1=seq(0, 30, length=100),
speed2=seq(0, 30, length=100)^2))
Conf <- predict(fm, Z0, interval='confidence')
Pred <- predict(fm, Z0, interval='prediction')
plot(cars, xlab='Speed', ylab='Stopping distance', las=1, xlim=c(0,30), ylim=c(-45,150))
lines(Z0[,1], Conf[,'fit'])
lines(Z0[,1], Conf[,'lwr'], lty=2, col='red', lwd=2)
lines(Z0[,1], Conf[,'upr'], lty=2, col='red', lwd=2)
lines(Z0[,1], Pred[,'lwr'], lty=3, col='gold', lwd=2)
lines(Z0[,1], Pred[,'upr'], lty=3, col='gold', lwd=2)
## Model diagnostic ---------------------------------------------------------------------------
# Verify assumptions
par(mfrow=c(2,2))
plot(fm)
Q <- cbind(quakes[,1:2], depth=-quakes[,3]/100)
d <- dist(Q)
clusterw <- cutree(hclust(d, method='ward.D2'), 2)
open3d()
par3d(windowRect=c(680,40,1350,720))
points3d(x=Q$lat, y=Q$long, z=Q$depth, size=4, col=clusterw+1, aspect = T)
box3d()
axes3d()
## Model 1: No factor effect ------------------------------------------------------------------
# depth = beta0 + beta1*lat + beta2*long + eps
fit  <- lm(depth ~ lat + long, data=Q)
summary(fit)
open3d()
par3d(windowRect=c(680,40,1350,720))
points3d(x=Q$lat, y=Q$long, z=Q$depth, size=4, col=clusterw+1, aspect = T)
box3d()
axes3d()
points3d(x=Q$lat, y=Q$long, z=fitted(fit), size=4, col = 'blue')
surface3d(range(Q$lat), range(Q$long),
matrix(predict(fit, expand.grid(lat=range(Q$lat), long=range(Q$long))),2,2),
alpha = 0.5)
par(mfrow=c(2,2))
plot(fit)
plot(fit)
shapiro.test(rstandard(fit))
## Model 2: Factor effect ---------------------------------------------------------------------
# Introduce a dummy variable indicating the belonging to a group
dummy <- clusterw - 1   # 0 = red
Qd <- cbind(Q, dummy)
head(Qd)
fitd <- lm(depth ~ lat + long + dummy + lat:dummy + long:dummy, data=Qd)
# or lm(depth ~ long*dummy + lat*dummy, data=Qd)
summary(fitd)
# Fitted model:
open3d()
par3d(windowRect=c(680,40,1350,720))
points3d(x=Q$lat, y=Q$long, z=Q$depth, size=4, col=clusterw+1, aspect = T)
points3d(x=Qd$lat, y=Qd$long, z=fitted(fitd), size=4, col = 'blue')
surface3d(range(Q$lat), range(Q$long),
matrix(predict(fitd, expand.grid(lat=range(Q$lat), long=range(Q$long), dummy=c(1))),2,2),
alpha = 0.5, col='green')
surface3d(range(Q$lat), range(Q$long),
matrix(predict(fitd, expand.grid(lat=range(Q$lat), long=range(Q$long), dummy=c(0))),2,2),
alpha = 0.5, col='red')
box3d()
axes3d()
box3d()
axes3d()
# Residuals:
par(mfrow=c(2,2))
plot(fitd)
# Fitted model:
open3d()
par3d(windowRect=c(680,40,1350,720))
points3d(x=Q$lat, y=Q$long, z=Q$depth, size=4, col=clusterw+1, aspect = T)
points3d(x=Qd$lat, y=Qd$long, z=fitted(fitd), size=4, col = 'blue')
surface3d(range(Q$lat), range(Q$long),
matrix(predict(fitd, expand.grid(lat=range(Q$lat), long=range(Q$long), dummy=c(1))),2,2),
alpha = 0.5, col='green')
surface3d(range(Q$lat), range(Q$long),
matrix(predict(fitd, expand.grid(lat=range(Q$lat), long=range(Q$long), dummy=c(0))),2,2),
alpha = 0.5, col='red')
box3d()
axes3d()
# Residuals:
par(mfrow=c(2,2))
plot(fitd)
shapiro.test(rstandard(fitd))
# test: are the two planes needed?
A <- rbind(c(0,0,0,1,0,0), c(0,0,0,0,1,0), c(0,0,0,0,0,1))
b <- c(0,0,0)
linearHypothesis(fitd, A, b)
# Reduce the model:
summary(fitd)
fitD <- lm(depth ~ lat + long + dummy + long:dummy, data=Qd)
summary(fitD)
# Fitted model
open3d()
par3d(windowRect=c(680,40,1350,720))
points3d(x=Q$lat, y=Q$long, z=Q$depth, size=4, col=clusterw+1, aspect = T)
axes3d()
points3d(x=Qd$lat, y=Qd$long, z=fitted(fitD), size=4, col = 'blue')
surface3d(range(Q$lat), range(Q$long),
matrix(predict(fitD, expand.grid(lat = range(Q$lat), long = range(Q$long), dummy=c(1))),2,2),
alpha = 0.5, col='green')
surface3d(range(Q$lat), range(Q$long),
matrix(predict(fitD, expand.grid(lat = range(Q$lat), long = range(Q$long), dummy=c(0))),2,2),
alpha = 0.5, col='red')
# Residuals:
par(mfrow=c(2,2))
plot(fitD)
# Residuals:
par(mfrow=c(2,2))
plot(fitD)
shapiro.test(rstandard(fitD))
# generation of training set and test set
set.seed(1)
# true model: regressors x, x^2, x^3; coefficients (1,1,1,-1)
f <- function(x){1+x+x^2-x^3}
sigma <- 0.25
x <- seq(-1, 1.5, length = 21)
y <- f(x) + rnorm(21, sd = sigma)
y.new <- f(x) + rnorm(21, sd = sigma)
setwd("C:/Users/Ascolani/Desktop/APPLIED STATISTICS/LABS/8_LM1")
library(MASS)
library(car)
library(rgl)
options(rgl.printRglwidget = TRUE)
# Example 1: Multiple linear regression -------------------------------------------------------
# Dataset cars: distance taken to stop [ft] as a function of velocity [mph]
# for some cars in the 1920s
help(cars)
head(cars)
dim(cars)
plot(cars, xlab='Speed', ylab='Stopping distance', las=1)
n          <- dim(cars)[[1]]
distance   <- cars$dist
speed1     <- cars$speed
speed2     <- cars$speed^2
## Parameter estimation -----------------------------------------------------------------------
# Assumptions: E(Eps) = 0  and  Var(Eps) = sigma^2
help(lm)
fm <- lm(distance ~ speed1 + speed2)
summary(fm)
fitted(fm)        # y hat
residuals(fm)     # eps hat
coefficients(fm)  # beta_i
vcov(fm)          # cov(beta_i)
fm$rank # order of the model [r+1]
fm$df   # degrees of freedom of the residuals [n-(r+1)]
hatvalues(fm) # h_ii (or sometimes called "leverage")
rstandard(fm) # standardized residuals: eps_j / sqrt(s^2*(1-h_ii))
sum(residuals(fm)^2)/fm$df  # s^2 estimate of sigma^2
plot(cars, xlab='Speed', ylab='Stopping distance', las=1, xlim=c(0,30), ylim=c(-5,130))
x <- seq(0,30,by=0.1)
b <- coef(fm)
lines(x, b[1]+b[2]*x+b[3]*x^2)
## Inference on the parameters ----------------------------------------------------------------
# Assumption: Eps ~ N(0, sigma^2)
# Test (Fisher):
#   H0: (beta1, beta2) == (0, 0) vs H1: (beta1, beta2) != (0, 0)
linearHypothesis(fm, rbind(c(0,1,0), c(0,0,1)), c(0,0))
rbind(c(0,1,0), c(0,0,1)), c(0,0)
rbind(c(0,1,0), c(0,0,1)), c(0,0))
summary(fm)
p <- 2  # number of tested coefficients
r <- 2  # number of regressors
# Confidence region:
# center: point estimate
c(coefficients(fm)[2], coefficients(fm)[3])
# Direction of the axes?
eigen(vcov(fm)[2:3,2:3])$vectors
plot(coefficients(fm)[2], coefficients(fm)[3], xlim = c(-6,6), ylim = c(-6,6), asp=1, xlab='beta1', ylab='beta2')
ellipse(coefficients(fm)[2:3], vcov(fm)[2:3,2:3], sqrt(p*qf(1-0.05,p,n-(r+1))))
abline(v=0)
abline(h=0)
# Bonferroni intervals (level 95%)
Bf <- rbind(
beta1=c(coefficients(fm)[2]-sqrt(vcov(fm)[2,2])*qt(1-0.05/(2*p), n-(r+1)),
coefficients(fm)[2]+sqrt(vcov(fm)[2,2])*qt(1-0.05/(2*p), n-(r+1))),
beta2=c(coefficients(fm)[3]-sqrt(vcov(fm)[3,3])*qt(1-0.05/(2*p), n-(r+1)),
coefficients(fm)[3]+sqrt(vcov(fm)[3,3])*qt(1-0.05/(2*p), n-(r+1)))
)
Bf
# or (only for intervals on beta)
confint(fm, level= 1-0.05/p)[2:3,]  # Bonferroni correction!
### Test:
# H0: (beta0+beta2, beta1) == (0,0) vs H1: (beta0+beta2, beta1) != (0,0)
C <- rbind(c(1,0,1), c(0,1,0))
linearHypothesis(fm, C, c(0,0))
## Prediction ---------------------------------------------------------------------------------
# Command predict()
Z0.new <- data.frame(speed1=10, speed2=10^2)
# Conf. int. for the mean
Conf <- predict(fm, Z0.new, interval='confidence', level=1-0.05)
Conf
# Pred. int. for a new obs
Pred <- predict(fm, Z0.new, interval='prediction', level=1-0.05)
Pred
plot(cars, xlab='Speed', ylab='Stopping distance', las=1, xlim=c(0,30), ylim=c(-10,130))
x <- seq(0,30,by=0.1)
b <- coef(fm)
lines(x, b[1]+b[2]*x+b[3]*x^2)
points(10,Conf[1], pch=19)
segments(10,Pred[2],10,Pred[3],col='gold', lwd=2)
segments(10,Conf[2],10,Conf[3],col='red', lwd=2)
points(10,Conf[2], pch='-', col='red', lwd=2)
points(10,Conf[3], pch='-', col='red', lwd=2)
points(10,Pred[2], pch='-', col='gold', lwd=2)
points(10,Pred[3], pch='-', col='gold', lwd=2)
# We can repeat these for values of speed between 0 and 30
# (point-wise intervals!)
Z0   <- data.frame(cbind(speed1=seq(0, 30, length=100),
speed2=seq(0, 30, length=100)^2))
Conf <- predict(fm, Z0, interval='confidence')
Pred <- predict(fm, Z0, interval='prediction')
plot(cars, xlab='Speed', ylab='Stopping distance', las=1, xlim=c(0,30), ylim=c(-45,150))
lines(Z0[,1], Conf[,'fit'])
lines(Z0[,1], Conf[,'lwr'], lty=2, col='red', lwd=2)
lines(Z0[,1], Conf[,'upr'], lty=2, col='red', lwd=2)
lines(Z0[,1], Pred[,'lwr'], lty=3, col='gold', lwd=2)
lines(Z0[,1], Pred[,'upr'], lty=3, col='gold', lwd=2)
## Model diagnostic ---------------------------------------------------------------------------
# Verify assumptions
par(mfrow=c(2,2))
plot(fm)
shapiro.test(residuals(fm))
# What happens if we change unit of measure?
# Exercise: Compare the results obtained from:
distance.m      <- cars$dist*0.3         # feet -> m
speed1.kmh      <- cars$speed*1.6        # miles/hour -> km per hour
speed2.kmh2     <- cars$speed^2 * 1.6^2
anscombe
attach(anscombe)
# dataset 1
lm1 <- lm(y1~ x1)
summary(lm1)
# dataset 2
lm2 <- lm(y2~ x2)
summary(lm2)
# dataset 3
lm3 <- lm(y3~ x3)
summary(lm3)
# dataset 4
lm4 <- lm(y4~ x4)
summary(lm4)
# same R^2, same coefficient estimate, same residual std error
par(mfcol=c(2,4))
plot(x1,y1, main='Dataset 1')
abline(lm1)
plot(x1,residuals(lm1))
abline(h=0)
plot(x2,y2, main='Dataset 2')
abline(lm2)
plot(x2,residuals(lm2))
abline(h=0)
plot(x3,y3, main='Dataset 3')
abline(lm3)
plot(x3,residuals(lm3))
abline(h=0)
plot(x4,y4, main='Dataset 4')
abline(lm4)
plot(x4,residuals(lm4))
abline(h=0)
dev.off()
detach(anscombe)
data <- read.table('brain_weight.txt', header=T)
head(data)
dim(data)
dimnames(data)
attach(data)
X <- body
Y <- brain
attach(data)
X <- body
Y <- brain
detach(data)
par(mfrow=c(1,1))
plot(X, Y, main='Scatterplot brain weight vs body weight', lwd=2,
xlab='Body weight', ylab='Brain weight')
plot(X, Y, main='Scatterplot brain weight vs body weight', lwd=2,
xlab='Body weight', ylab='Brain weight',col='white',xlim=c(-1000,8000))
text(X, Y,dimnames(data)[[1]],cex=1)
result <- lm(Y ~ X)
summary(result)
coef <- result$coef
plot(X, Y, main='Scatterplot brain weight vs body weight', lwd=2,
xlab='Body weight', ylab='Brain weight')
abline(coef[1],coef[2], lwd=2,col='red')
# diagnostics of the residuals
par(mfrow=c(2,2))
plot(result)
shapiro.test(residuals(result))
par(mfrow=c(1,1))
# exclude outliers?
plot(X, Y, main='Scatterplot brain weight vs body weight', lwd=2,
xlab='Body weight', ylab='Brain weight',col='white',xlim=c(-1000,8000))
text(X, Y,dimnames(data)[[1]],cex=1)
abline(h=3000)
# diagnostics of the residuals
par(mfrow=c(2,2))
plot(result)
# diagnostics of the residuals
par(mfrow=c(2,2))
plot(result)
par(mfrow=c(1,1))
# exclude outliers?
plot(X, Y, main='Scatterplot brain weight vs body weight', lwd=2,
xlab='Body weight', ylab='Brain weight',col='white',xlim=c(-1000,8000))
text(X, Y,dimnames(data)[[1]],cex=1)
abline(h=3000)
plot(X[which(Y<3000)], Y[which(Y<3000)], main='Scatterplot brain weight vs body weight', lwd=2,
xlab='Body weight', ylab='Brain weight',col='white',xlim=c(-100,600))
text(X[which(Y<3000)], Y[which(Y<3000)],dimnames(data)[[1]],cex=1)
log.X <- log(X)
log.Y <- log(Y)
plot(log.X, log.Y, main='Scatterplot ln(Brain weight) vs ln(Body weight)', lwd=2,
xlab='ln(Body weight)', ylab='ln(Brain weight)')
result.log <- lm(log.Y ~ log.X)
summary(result.log)
coef.log= result.log$coef
abline(coef.log[1],coef.log[2], lwd=2,col='red')
# diagnostics of the residuals
par(mfrow=c(2,2))
plot(result.log)
shapiro.test(residuals(result.log))
par(mfrow=c(1,2))
# confidence intervals and prediction intervals
plot(log.X, log.Y, main='Scatterplot ln(Brain weight) vs ln(Body weight)', lwd=2,
xlab='ln(Body weight)', ylab='ln(Brain weight)')
X.new.log <- data.frame(log.X = seq(min(log.X), max(log.X), len=100))
IC.log <-predict(result.log ,X.new.log,interval="confidence",level=0.95)
matplot(X.new.log,IC.log,add=T,type='l',col=c('black','blue','blue'),lwd=2,lty=2)
IP.log <-predict(result.log ,X.new.log,interval="prediction",level=0.95)
matplot(X.new.log,IP.log,add=T,type='l',col=c('black','green','green'),lwd=2,lty=2)
legend('topleft', legend=c('regression line','confidence intervals','prediction intervals'),
col=c('black','blue','green'), lwd=2, cex=0.85)
# plot on the original data
plot(X, Y, main='Scatterplot Brain weight vs Body weight', lwd=2,
xlab='Body weight', ylab='Brain weight')
IC <- exp(IC.log)
IP <- exp(IP.log)
X.new <- exp(X.new.log)
matplot(X.new,IC,add=T,type='l',col=c('black','blue','blue'),lwd=2,lty=2)
matplot(X.new,IP,add=T,type='l',col=c('black','green','green'),lwd=2,lty=2)
Q <- cbind(quakes[,1:2], depth=-quakes[,3]/100)
d <- dist(Q)
clusterw <- cutree(hclust(d, method='ward.D2'), 2)
open3d()
par3d(windowRect=c(680,40,1350,720))
points3d(x=Q$lat, y=Q$long, z=Q$depth, size=4, col=clusterw+1, aspect = T)
box3d()
axes3d()
open3d()
par3d(windowRect=c(680,40,1350,720))
points3d(x=Q$lat, y=Q$long, z=Q$depth, size=4, col=clusterw+1, aspect = T)
box3d()
axes3d()
## Model 1: No factor effect ------------------------------------------------------------------
# depth = beta0 + beta1*lat + beta2*long + eps
fit  <- lm(depth ~ lat + long, data=Q)
summary(fit)
open3d()
par3d(windowRect=c(680,40,1350,720))
points3d(x=Q$lat, y=Q$long, z=Q$depth, size=4, col=clusterw+1, aspect = T)
box3d()
axes3d()
points3d(x=Q$lat, y=Q$long, z=fitted(fit), size=4, col = 'blue')
surface3d(range(Q$lat), range(Q$long),
matrix(predict(fit, expand.grid(lat=range(Q$lat), long=range(Q$long))),2,2),
alpha = 0.5)
par(mfrow=c(2,2))
plot(fit)
shapiro.test(rstandard(fit))
## Model 2: Factor effect ---------------------------------------------------------------------
# Introduce a dummy variable indicating the belonging to a group
dummy <- clusterw - 1   # 0 = red
## Model 2: Factor effect ---------------------------------------------------------------------
# Introduce a dummy variable indicating the belonging to a group
dummy <- clusterw - 1   # 0 = red
Qd <- cbind(Q, dummy)
head(Qd)
fitd <- lm(depth ~ lat + long + dummy + lat:dummy + long:dummy, data=Qd)
# or lm(depth ~ long*dummy + lat*dummy, data=Qd)
summary(fitd)
# test: are the two planes needed?
A <- rbind(c(0,0,0,1,0,0), c(0,0,0,0,1,0), c(0,0,0,0,0,1))
b <- c(0,0,0)
linearHypothesis(fitd, A, b)
# Reduce the model:
summary(fitd)
fitD <- lm(depth ~ lat + long + dummy + long:dummy, data=Qd)
summary(fitD)
# Fitted model
open3d()
par3d(windowRect=c(680,40,1350,720))
points3d(x=Q$lat, y=Q$long, z=Q$depth, size=4, col=clusterw+1, aspect = T)
axes3d()
points3d(x=Qd$lat, y=Qd$long, z=fitted(fitD), size=4, col = 'blue')
surface3d(range(Q$lat), range(Q$long),
matrix(predict(fitD, expand.grid(lat = range(Q$lat), long = range(Q$long), dummy=c(1))),2,2),
alpha = 0.5, col='green')
surface3d(range(Q$lat), range(Q$long),
matrix(predict(fitD, expand.grid(lat = range(Q$lat), long = range(Q$long), dummy=c(0))),2,2),
alpha = 0.5, col='red')
# Residuals:
par(mfrow=c(2,2))
plot(fitD)
shapiro.test(rstandard(fitD))
# generation of training set and test set
set.seed(1)
# true model: regressors x, x^2, x^3; coefficients (1,1,1,-1)
f <- function(x){1+x+x^2-x^3}
sigma <- 0.25
x <- seq(-1, 1.5, length = 21)
y <- f(x) + rnorm(21, sd = sigma)
y.new <- f(x) + rnorm(21, sd = sigma)
# build design matrix
data <- NULL
for(p in 0:20)
data <- cbind(data, x^p)
colnames(data) <- c(paste('x', 0:20, sep=''))
data <- data.frame(data)
head(data)
dim(data)
# grid to plot
data.plot <- NULL
x.plot <- seq(-1, 1.5, length = 210)
for(p in 0:20)
data.plot <- cbind(data.plot, x.plot^p)
colnames(data.plot) <- c(paste('x', 0:20, sep=''))
data.plot <- data.frame(data.plot)
# plot of the training set, test set and "true" mean curve
par(mfrow=c(1,1))
plot(x, y, pch=20)                            # training set
points(x, y.new, col='red')                   # test set
lines(x.plot, f(x.plot), col='blue', lty=2)   # true mean
legend('bottomright', legend=c('Training Set', 'Test Set', 'True Mean'),
col=c('black', 'red', 'blue'), pch=c(20, 1, NA), lty=c(NA, NA, 2))
# regression with polynomials of increasing order
SSres <- SSres.new <- s2 <- b <- R2 <- R2.adj <- NULL
n <- 21
par(mfrow=c(4,4), mar=rep(2,4))
for(p in 1:16)
{
fit <- lm(y ~ 0 + . , data[,1:(p+1)])
plot(x, y, pch=20, main=sprintf('%dth order', p))
points(x, y.new, col='red')
lines(x.plot, predict(fit, data.plot))
lines(x.plot, f(x.plot), col='blue', lty=2)
SSres <- c(SSres, sum((y - fitted(fit))^2))
SSres.new <- c(SSres.new, sum((y.new - fitted(fit))^2))
s2 <- c(s2, sum((y - fitted(fit))^2)/(n - (p+1)))
R2 <- c(R2, summary(fit)$r.squared)
R2.adj <- c(R2.adj, summary(fit)$adj.r.squared)
bp <- rep(0,17)
bp[1:(p+1)] <- coefficients(fit)
b <- cbind(b, bp)
}
par(mfrow=c(2,2))
