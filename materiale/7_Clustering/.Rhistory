par(mfrow=c(1,3))
plot(x.es, main = 'Single linkage'  , hang=-0.1, xlab='', labels=F, sub='')
plot(x.ea, main = 'Average linkage' , hang=-0.1, xlab='', labels=F, sub='')
plot(x.ec, main = 'Complete linkage', hang=-0.1, xlab='', labels=F, sub='')
par(mfrow=c(1,1))
# let's cut the tree as to get 2 clusters
cluster.es <- cutree(x.es, k = 2)
cluster.ea <- cutree(x.ea, k = 2)
cluster.ec <- cutree(x.ec, k = 2)
par(mfrow=c(1,3))
plot(X, xlab='Var 1', ylab='Var 2', main = 'Single linkage',
col=ifelse(cluster.es==1,'red','blue'), pch=16, asp=1)
plot(X, xlab='Var 1', ylab='Var 2', main = 'Average linkage',
col=ifelse(cluster.ea==1,'red','blue'), pch=16, asp=1)
plot(X, xlab='Var 1', ylab='Var 2', main = 'Complete linkage',
col=ifelse(cluster.ec==1,'red','blue'), pch=16, asp=1)
par(mfrow=c(1,1))
### Bivariate case (example of ellipsoidal clusters) ------------------------------------------
p <- 2
n <- 100
mu1 <- c(0,1)
mu2 <- c(6.5,1)
e1 <- c(1,1)
e2 <- c(-1,1)
sig <- 5*cbind(e1)%*%rbind(e1)+.1*cbind(e2)%*%rbind(e2)
set.seed(2)
X1 <- rmvnorm(n, mu1, sig)
X2 <- rmvnorm(n, mu2, sig)
X <- rbind(X1, X2)
# If we knew the labels:
plot(X, xlab='Var 1', ylab='Var 2', asp=1, col=rep(c('red','blue'),each=100), pch=19)
# How we actually see the data:
plot(X, xlab='Var 1', ylab='Var 2', asp=1, pch=19)
## compare results with different linkage, when Euclidean distances is used
x.d <- dist(X, method = 'euclidean')
x.es <- hclust(x.d, method='single')
x.ea <- hclust(x.d, method='average')
x.ec <- hclust(x.d, method='complete')
par(mfrow=c(1,3))
plot(x.es, main = 'Single linkage', ylab='Euclidean distance', hang=-0.1, xlab='', labels=F, sub='')
plot(x.ea, main = 'Average linkage', hang=-0.1, xlab='', labels=F, sub='')
plot(x.ec, main = 'Complete linkage', hang=-0.1, xlab='', labels=F, sub='')
# let's cut the tree to get 2 clusters
par(mfrow=c(1,3))
plot(x.es, main = 'Single linkage', ylab='Euclidean distance', hang=-0.1, xlab='', labels=F, sub='')
rect.hclust(x.es, k=2)
plot(x.ea, main = 'Average linkage', hang=-0.1, xlab='', labels=F, sub='')
rect.hclust(x.ea, k=2)
plot(x.ec, main = 'Complete linkage', hang=-0.1, xlab='', labels=F, sub='')
rect.hclust(x.ec, k=2)
cluster.es <- cutree(x.es, k = 2)
cluster.ea <- cutree(x.ea, k = 2)
cluster.ec <- cutree(x.ec, k = 2)
par(mfrow=c(1,3))
plot(X, xlab='Var 1', ylab='Var 2', main = 'Euclidean, Single linkage', col=cluster.es+1, pch=16,
asp=1)
plot(X, xlab='Var 1', ylab='Var 2', main = 'Euclidean, Average linkage', col=cluster.ea+1, pch=16,
asp=1)
plot(X, xlab='Var 1', ylab='Var 2', main = 'Euclidean, Complete linkage', col=cluster.ec+1, pch=16,
asp=1)
help(quakes)
head(quakes)
dim(quakes)
Q <- cbind(quakes[,1:2], depth = -quakes[,3]/100)
head(Q)
plot3d(Q, size=3, col='orange', aspect = F)
# dissimilarity matrix (Euclidean metric)
d <- dist(Q)
image(as.matrix(d))
clusts <- hclust(d, method='single')
clusta <- hclust(d, method='average')
clustc <- hclust(d, method='complete')
clustw <- hclust(d, method='ward.D2')
plot(clusta, hang=-0.1, labels=FALSE, main='average', xlab='', sub='')
plot(clustc, hang=-0.1, labels=FALSE, main='complete', xlab='', sub='')
plot(clustw, hang=-0.1, labels=FALSE, main='ward', xlab='', sub='')
par(mfrow=c(2,2))
plot(clusts, hang=-0.1, labels=FALSE, main='single', xlab='', sub='')
plot(clusta, hang=-0.1, labels=FALSE, main='average', xlab='', sub='')
plot(clustc, hang=-0.1, labels=FALSE, main='complete', xlab='', sub='')
plot(clustw, hang=-0.1, labels=FALSE, main='ward', xlab='', sub='')
open3d()
# single linkage
clusters <- cutree(clusts, 2)
plot3d(Q, size=3, col=clusters+1, aspect = F)
clusters <- cutree(clusts, 3)
plot3d(Q, size=3, col=clusters+1, aspect = F)
# average linkage
clustera <- cutree(clusta, 2)
plot3d(Q, size=3, col=clustera+1, aspect = F)
clustera <- cutree(clusta, 3)
plot3d(Q, size=3, col=clustera+1, aspect = F)
# complete linkage
clusterc <- cutree(clustc, 2)
plot3d(Q, size=3, col=clusterc+1, aspect = F)
clusterc <- cutree(clustc, 3)
plot3d(Q, size=3, col=clusterc+1, aspect = F)
# ward linkage
clusterw <- cutree(clustw, 2)
plot3d(Q, size=3, col=clusterw+1, aspect = F)
clusterw <- cutree(clustw, 3)
plot3d(Q, size=3, col=clusterw+1, aspect = F)
# simulated data
n <- 100
set.seed(1)
x <- matrix(rnorm(n*2), ncol=2)
x[1:(n/2),1] <- x[1:(n/2),1]+2
x[1:(n/2),2] <- x[1:(n/2),2]-2
plot(x,pch=20,cex=2,xlab='x1',ylab='x2')
par(mfrow=c(1,1))
plot(x,pch=20,cex=2,xlab='x1',ylab='x2')
k <- 2
cluster <- sample(1:2, n, replace=TRUE)
iter.max <- 3
colplot <- c('royalblue','red')
colpoints <- c('blue4','red4')
par(mfrow = c(iter.max,3))
for(i in 1:iter.max){
C <- NULL
for(l in 1:k)
C <- rbind(C, colMeans(x[cluster == l,]))
plot(x, col = colplot[cluster],pch=19)
line <- readline()
points(C, col = colpoints, pch = 4, cex = 2, lwd = 2)
line <- readline()
plot(x, col = 'grey',pch=19)
points(C, col = colpoints, pch = 4, cex = 2, lwd = 2)
line <- readline()
QC <- rbind(C, x)
Dist <- as.matrix(dist(QC, method = 'euclidean'))[(k+1):(k+n),1:k]
for(j in 1:n)
cluster[j] <- which.min(Dist[j,])
plot(x, col = colplot[cluster],pch=19)
points(C, col = colpoints, pch = 4, cex = 2, lwd = 2)
line <- readline()
}
## Example: Earthquakes dataset --------------------------------------------------------------
# in automatic, command kmeans()
help(kmeans)
result.k <- kmeans(Q, centers=2) # Centers: fixed number of clusters
names(result.k)
result.k$cluster      # labels of clusters
result.k$centers      # centers of the clusters
result.k$totss        # tot. sum of squares
result.k$withinss     # sum of squares within clusters
result.k$tot.withinss # sum(sum of squares within cluster)
result.k$betweenss    # sum of squares between clusters
result.k$size         # dimension of the clusters
plot(Q, col = result.k$cluster+1)
open3d()
plot3d(Q, size=3, col=result.k$cluster+1, aspect = F)
points3d(result.k$centers,size=10)
# method 1)
b <- NULL
w <- NULL
for(k in 1:10){
result.k <- kmeans(Q, k)
w <- c(w, sum(result.k$wit))
b <- c(b, result.k$bet)
}
matplot(1:10, w/(w+b), pch='', xlab='clusters', ylab='within/tot', main='Choice of k', ylim=c(0,1))
par(mfrow=c(1,1))
matplot(1:10, w/(w+b), pch='', xlab='clusters', ylab='within/tot', main='Choice of k', ylim=c(0,1))
lines(1:10, w/(w+b), type='b', lwd=2)
# this method seems to suggest k = 2 or 4
# let's try also k=4:
result.k <- kmeans(Q, 4)
plot(Q, col = result.k$cluster+1)
open3d()
plot3d(Q, size=3, col=result.k$cluster+1, aspect = F)
# Simulate the data
set.seed(2)
n <- 400
x <- cbind(x = runif(4) + rnorm(n, sd = 0.1), y = runif(4) + rnorm(n, sd = 0.1))
true_clusters <- rep(1:4, time = 100)
plot(x, col = true_clusters, pch = 19)
plot(x, pch = 19)
# How to choose eps from minPts?
# Plot of the distances to the minPts-1 nearest neighbor
kNNdistplot(x, minPts = minPts)
library(dbscan)
# Choice of hyperparameters for DBSCAN
# Rule of thumb, minPts should be at least p + 1 = 3 here
# Can be eventually increased
minPts <- 3
# How to choose eps from minPts?
# Plot of the distances to the minPts-1 nearest neighbor
kNNdistplot(x, minPts = minPts)
# Taking eps = 0.05 seems to be a good threshold
abline(h = 0.05, col = "red", lty = 2)
# Run the dbscan
dbs <- dbscan(x, eps = 0.05, minPts = 3)
dbs
# Plot of the resulting clustering
plot(x, col = dbs$cluster + 1L, pch=19)
# Silhouette score (from the package "cluster")
help(silhouette)
library(cluster)
# Silhouette score (from the package "cluster")
help(silhouette)
# Let's compute the silhouette score on the clustering performed before
# WARNING (specific to DBSCAN): We need to remove the noise points as they do
# not belong to a cluster, before computing the silhouette score
clustered_index <- which(dbs$cluster != 0) # Index of non noise points
clustered_points <- x[clustered_index] # only clustered points
clustered_labels <- dbs$cluster[clustered_index] # corresponding labels
sil <- silhouette(clustered_labels, dist(clustered_points))
summary(sil)
sil_score <- function(labels, dist) {
# Compute the average of the silhouette widths
sil <- silhouette(labels, dist)
sil_widths <- sil[,"sil_width"]
mean(sil_widths)
}
sil_score(clustered_labels, dist(clustered_points))
# Grid Search
minPts_grid <- 1:20
eps_grid <- seq(0.01, 0.2, by = 0.01)
max_share_noise <- 0.2
dbscan_perf <- function(minPts, eps) {
# Compute the silhouette score resulting from dbscan clustering
dbs <- dbscan(x, eps, minPts) # Run dbscan
clustered_index <- which(dbs$cluster != 0) # Index of non noise points
clustered_points <- x[clustered_index] # only clustered points
clustered_labels <- dbs$cluster[clustered_index] # corresponding labels
nb_clusters <- length(unique(clustered_labels))
if ((nb_clusters > 1 & nb_clusters < n) & (length(which(dbs$cluster == 0))/n < max_share_noise)) {
# Silhouette score is defined only if 2 <= nb_clusters <= n-1
sil_score(clustered_labels, dist(clustered_points))
}
else {
# otherwise we return 0 which would be the approx. value of the silhouette
# score if the clusters were completely overlapping
0
}
}
# We compute the silhouette score for all combinations of minPts and eps
perf_grid <- outer(minPts_grid, eps_grid, FUN = Vectorize(dbscan_perf))
dimnames(perf_grid) <- list(minPts_grid, eps_grid)
# Histogram of the Silhouette scores
hist(perf_grid, breaks = 20, xlab = "Silhouette score", xlim = c(-1, 1), main = NULL)
max_score <- max(perf_grid)
min_score <- min(perf_grid)
max_abs <- max(abs(max_score), abs(min_score))
image.plot(x = eps_grid, y = minPts_grid, z = perf_grid, xlab = "eps", ylab = "minPts",
main = 'Silhouette score', col = hcl.colors(64, palette = 'Blue-Red'),
breaks = c(seq(-max_abs, 0, length=33)[-33], seq(0, max_abs, length=33)))
library(fields)
image.plot(x = eps_grid, y = minPts_grid, z = perf_grid, xlab = "eps", ylab = "minPts",
main = 'Silhouette score', col = hcl.colors(64, palette = 'Blue-Red'),
breaks = c(seq(-max_abs, 0, length=33)[-33], seq(0, max_abs, length=33)))
# Retrieve best parameter values
max_score <- max(perf_grid)
argmax_score <- which(perf_grid == max_score, arr.ind = TRUE)
best_eps <- eps_grid[argmax_score[2]]
best_minPts <- minPts_grid[argmax_score[1]]
best_eps
best_minPts
max_score
# Run the dbscan
dbs <- dbscan(x, best_eps, best_minPts)
dbs
plot(x, col = dbs$cluster + 1L, pch=19)
# Let's try now with eps = 0.09 and minPts = 15
dbs <- dbscan(x, eps = 0.09, minPts = 15)
dbs
# Recovered the original clusters!
plot(x, col = dbs$cluster + 1L, pch=19)
# Example of dataset where DBSCAN succeed and k-means & hierarchical fail
data("moons")
plot(moons, pch=19)
minPts = 3 # Dimensionality + 1
kNNdistplot(moons, minPts = 3)
abline(h = 0.27, col = "red", lty = 2)
eps <- 0.3
minPts <- 3
dbs <- dbscan(moons, eps, minPts)
dbs
plot(moons, col = dbs$cluster + 1L, pch=19)
# Let's try the other algorithms that we saw:
hclust.s <- hclust(dist(moons), method='single')
hclust.a <- hclust(dist(moons), method='average')
hclust.c <- hclust(dist(moons), method='complete')
# plot of the dendrograms
par(mfrow=c(1,3))
plot(hclust.s, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(hclust.c, main='euclidean-complete', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(hclust.a, main='euclidean-average', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
cluster.c <- cutree(hclust.c, k=2) # euclidean-complete:
cluster.s <- cutree(hclust.s, k=3) # euclidean-single
cluster.a <- cutree(hclust.a, k=3) # euclidean-average
par(mfrow=c(1,3))
plot(moons, col=cluster.c + 1L, pch=19, main='Complete')
plot(moons, col=cluster.a + 1L, pch=19, main='Average')
plot(moons, col=cluster.s + 1L, pch=19, main='Single')
# Choose k -> elbow method
b <- NULL
w <- NULL
for(k in 1:10){
k.means <- kmeans(moons, k)
w <- c(w, sum(k.means$wit))
b <- c(b, k.means$bet)
}
par(mfrow=c(1,1))
matplot(1:10, w/(w+b), pch='', xlab='clusters', ylab='within/tot', main='Choice of k', ylim=c(0,1))
lines(1:10, w/(w+b), type='b', lwd=2)
k.means <- kmeans(moons, centers = 4, nstart = 25)
plot(moons, col=k.means$cluster + 1L, pch=19, main='k-means')
# Example : European cities
help(eurodist)
eurodist
# R function for multidimensional scaling: cmdscale
help(cmdscale)
location <- cmdscale(eurodist, k=2)
location
# I have to set asp=1 (equal scales on the two axes)
# to correctly represent Euclidean distances
plot(location[,1], location[,2], type='n', asp=1, axes=FALSE, main="MDS of European cities",xlab='',ylab='')
text(location[,1], location[,2], labels=colnames(as.matrix(eurodist)), cex = 0.75, pos = 3)
# change the sign to get the North in the upper part of the plot
plot(location[,1], -location[,2], type='n', asp=1, axes=FALSE, main="MDS of European cities",xlab='',ylab='')
text(location[,1], -location[,2], labels=colnames(as.matrix(eurodist)), cex = 0.75, pos = 3)
# compare the original matrix d_ij = d(x_i,x_j) and delta_ij = d(y_i,y_j)
plot(eurodist, dist(location))
# visualize the most different distances
par(cex = 0.75, mar = c(10,10,2,2))
image(1:21, 1:21, asp=1, abs(as.matrix(dist(location)) - as.matrix(eurodist)), axes = F, xlab = '', ylab ='')
axis(1, at = 1:21, labels = colnames(as.matrix(eurodist)), las = 2, cex = 0.75)
axis(2, at = 1:21, labels = colnames(as.matrix(eurodist)), las = 1, cex = 0.75)
box()
# Rome-Athens
as.matrix(eurodist)[19,1]
as.matrix(dist(location))[19,1]
# Cologne-Geneve
as.matrix(eurodist)[6,8]
as.matrix(dist(location))[6,8]
Stressk <- NULL
for(k in 1:4)
{
location.k <- cmdscale(eurodist, k)
Stress <- (sum( (as.vector(eurodist) - as.vector(dist(location.k)))^2)  /
sum( as.vector(location.k)^2))^(1/2)
Stressk <- c(Stressk, Stress)
}
plot(1:4,Stressk,xlab='k',ylab='Stress',lwd=2)
set.seed(2)
n <- 400
x <- cbind(x = runif(4) + rnorm(n, sd = 0.1), y = runif(4) + rnorm(n, sd = 0.1))
true_clusters <- rep(1:4, time = 100)
plot(x, col = true_clusters, pch = 19)
plot(x, pch = 19)
# Choice of hyperparameters for DBSCAN
# Rule of thumb, minPts should be at least p + 1 = 3 here
# Can be eventually increased
minPts <- 3
library(dbscan)
# How to choose eps from minPts?
# Plot of the distances to the minPts-1 nearest neighbor
kNNdistplot(x, minPts = minPts)
# Taking eps = 0.05 seems to be a good threshold
abline(h = 0.05, col = "red", lty = 2)
# Run the dbscan
dbs <- dbscan(x, eps = 0.05, minPts = 3)
dbs
# Plot of the resulting clustering
plot(x, col = dbs$cluster + 1L, pch=19)
# Silhouette score (from the package "cluster")
help(silhouette)
# Let's compute the silhouette score on the clustering performed before
# WARNING (specific to DBSCAN): We need to remove the noise points as they do
# not belong to a cluster, before computing the silhouette score
clustered_index <- which(dbs$cluster != 0) # Index of non noise points
clustered_points <- x[clustered_index] # only clustered points
clustered_labels <- dbs$cluster[clustered_index] # corresponding labels
sil <- silhouette(clustered_labels, dist(clustered_points))
summary(sil)
sil_score <- function(labels, dist) {
# Compute the average of the silhouette widths
sil <- silhouette(labels, dist)
sil_widths <- sil[,"sil_width"]
mean(sil_widths)
}
sil_score(clustered_labels, dist(clustered_points))
# Grid Search
minPts_grid <- 1:20
eps_grid <- seq(0.01, 0.2, by = 0.01)
max_share_noise <- 0.2
max_share_noise <- 0.2
dbscan_perf <- function(minPts, eps) {
# Compute the silhouette score resulting from dbscan clustering
dbs <- dbscan(x, eps, minPts) # Run dbscan
clustered_index <- which(dbs$cluster != 0) # Index of non noise points
clustered_points <- x[clustered_index] # only clustered points
clustered_labels <- dbs$cluster[clustered_index] # corresponding labels
nb_clusters <- length(unique(clustered_labels))
if ((nb_clusters > 1 & nb_clusters < n) & (length(which(dbs$cluster == 0))/n < max_share_noise)) {
# Silhouette score is defined only if 2 <= nb_clusters <= n-1
sil_score(clustered_labels, dist(clustered_points))
}
else {
# otherwise we return 0 which would be the approx. value of the silhouette
# score if the clusters were completely overlapping
0
}
}
# We compute the silhouette score for all combinations of minPts and eps
perf_grid <- outer(minPts_grid, eps_grid, FUN = Vectorize(dbscan_perf))
dimnames(perf_grid) <- list(minPts_grid, eps_grid)
# Histogram of the Silhouette scores
hist(perf_grid, breaks = 20, xlab = "Silhouette score", xlim = c(-1, 1), main = NULL)
max_score <- max(perf_grid)
min_score <- min(perf_grid)
max_abs <- max(abs(max_score), abs(min_score))
max_abs
max_score
image.plot(x = eps_grid, y = minPts_grid, z = perf_grid, xlab = "eps", ylab = "minPts",
main = 'Silhouette score', col = hcl.colors(64, palette = 'Blue-Red'),
breaks = c(seq(-max_abs, 0, length=33)[-33], seq(0, max_abs, length=33)))
# Retrieve best parameter values
max_score <- max(perf_grid)
argmax_score <- which(perf_grid == max_score, arr.ind = TRUE)
best_eps <- eps_grid[argmax_score[2]]
best_minPts <- minPts_grid[argmax_score[1]]
best_eps
best_minPts
max_score
# Run the dbscan
dbs <- dbscan(x, best_eps, best_minPts)
dbs
plot(x, col = dbs$cluster + 1L, pch=19)
# Let's try now with eps = 0.09 and minPts = 15
dbs <- dbscan(x, eps = 0.09, minPts = 15)
dbs
# Recovered the original clusters!
plot(x, col = dbs$cluster + 1L, pch=19)
# Example of dataset where DBSCAN succeed and k-means & hierarchical fail
data("moons")
plot(moons, pch=19)
minPts = 3 # Dimensionality + 1
kNNdistplot(moons, minPts = 3)
abline(h = 0.27, col = "red", lty = 2)
eps <- 0.3
minPts <- 3
dbs <- dbscan(moons, eps, minPts)
dbs
plot(moons, col = dbs$cluster + 1L, pch=19)
# Let's try the other algorithms that we saw:
hclust.s <- hclust(dist(moons), method='single')
hclust.a <- hclust(dist(moons), method='average')
hclust.c <- hclust(dist(moons), method='complete')
# plot of the dendrograms
par(mfrow=c(1,3))
plot(hclust.s, main='euclidean-single', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(hclust.c, main='euclidean-complete', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(hclust.a, main='euclidean-average', hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
cluster.c <- cutree(hclust.c, k=2) # euclidean-complete:
cluster.s <- cutree(hclust.s, k=3) # euclidean-single
cluster.a <- cutree(hclust.a, k=3) # euclidean-average
par(mfrow=c(1,3))
plot(moons, col=cluster.c + 1L, pch=19, main='Complete')
plot(moons, col=cluster.a + 1L, pch=19, main='Average')
plot(moons, col=cluster.s + 1L, pch=19, main='Single')
# Choose k -> elbow method
b <- NULL
w <- NULL
for(k in 1:10){
k.means <- kmeans(moons, k)
w <- c(w, sum(k.means$wit))
b <- c(b, k.means$bet)
}
par(mfrow=c(1,1))
matplot(1:10, w/(w+b), pch='', xlab='clusters', ylab='within/tot', main='Choice of k', ylim=c(0,1))
lines(1:10, w/(w+b), type='b', lwd=2)
k.means <- kmeans(moons, centers = 4, nstart = 25)
plot(moons, col=k.means$cluster + 1L, pch=19, main='k-means')
# Example : European cities
help(eurodist)
eurodist
# R function for multidimensional scaling: cmdscale
help(cmdscale)
location <- cmdscale(eurodist, k=2)
location
# I have to set asp=1 (equal scales on the two axes)
# to correctly represent Euclidean distances
plot(location[,1], location[,2], type='n', asp=1, axes=FALSE, main="MDS of European cities",xlab='',ylab='')
text(location[,1], location[,2], labels=colnames(as.matrix(eurodist)), cex = 0.75, pos = 3)
# change the sign to get the North in the upper part of the plot
plot(location[,1], -location[,2], type='n', asp=1, axes=FALSE, main="MDS of European cities",xlab='',ylab='')
text(location[,1], -location[,2], labels=colnames(as.matrix(eurodist)), cex = 0.75, pos = 3)
# compare the original matrix d_ij = d(x_i,x_j) and delta_ij = d(y_i,y_j)
plot(eurodist, dist(location))
# visualize the most different distances
par(cex = 0.75, mar = c(10,10,2,2))
image(1:21, 1:21, asp=1, abs(as.matrix(dist(location)) - as.matrix(eurodist)), axes = F, xlab = '', ylab ='')
axis(1, at = 1:21, labels = colnames(as.matrix(eurodist)), las = 2, cex = 0.75)
axis(2, at = 1:21, labels = colnames(as.matrix(eurodist)), las = 1, cex = 0.75)
box()
# Rome-Athens
as.matrix(eurodist)[19,1]
as.matrix(dist(location))[19,1]
# Cologne-Geneve
as.matrix(eurodist)[6,8]
as.matrix(dist(location))[6,8]
Stressk <- NULL
for(k in 1:4)
{
location.k <- cmdscale(eurodist, k)
Stress <- (sum( (as.vector(eurodist) - as.vector(dist(location.k)))^2)  /
sum( as.vector(location.k)^2))^(1/2)
Stressk <- c(Stressk, Stress)
}
plot(1:4,Stressk,xlab='k',ylab='Stress',lwd=2)
